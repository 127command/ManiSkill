import copy
import json
from dataclasses import dataclass

import gymnasium as gym
import h5py
import numpy as np
import tyro
from tqdm import tqdm

import mani_skill.envs


@dataclass
class Args:
    runs_path: str
    out_dir: str


def main():
    args = tyro.cli(Args)

    import os
    from pathlib import Path

    # Dictionary to store paths for each environment experiment
    env_paths = {}

    # List all subfolders in runs_path
    for env_name in os.listdir(args.runs_path):
        env_dir = Path(args.runs_path) / env_name

        if not env_dir.is_dir():
            continue

        # Look for checkpoint and trajectory files
        ckpt_path = env_dir / "final_ckpt.pt"
        traj_path = env_dir / "test_videos" / "trajectory.h5"
        traj_metadata_path = env_dir / "test_videos" / "trajectory.json"

        # Only store if both files exist
        if not (ckpt_path.exists() and traj_path.exists()):
            print(
                f"Skipping {env_name} because checkpoint or trajectory file does not exist"
            )
            continue

        env_paths[env_name] = {
            "checkpoint": str(ckpt_path),
            "trajectory": str(traj_path),
            "metadata": str(traj_metadata_path),
        }
    print(env_paths)

    for env_name, env_path in env_paths.items():
        print(f"Processing {env_name}")
        # we need to truncate all success trajectories and remove all failed trajctories
        traj_path = env_path["trajectory"]
        file = h5py.File(traj_path, "r")
        metadata_path = env_path["metadata"]
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
        env_id = metadata["env_info"]["env_id"]
        new_metadata = copy.deepcopy(metadata)
        new_metadata["episodes"] = []

        out_trajectory_path = os.path.join(
            args.out_dir, f"{env_id}/rl/trajectory.state.pd_joint_delta_pos.cuda.h5"
        )
        os.makedirs(os.path.dirname(out_trajectory_path), exist_ok=True)
        out_file = h5py.File(out_trajectory_path, "w")

        failed_count = 0
        truncated_count = 0
        avg_episode_length = 0
        original_episode_count = len(metadata["episodes"])
        first_success_indexes = []
        for episode in tqdm(metadata["episodes"]):
            traj_id = f"traj_{episode['episode_id']}"
            traj = file[traj_id]
            success = np.array(traj["success"])

            if not success.any():
                # this failed
                failed_count += 1
                continue
            # truncate until last success
            success_indexes = success.nonzero()[0]
            last_success_index = int(success_indexes[-1])
            first_success_index = int(success_indexes[0])
            first_success_indexes.append(first_success_index)
            if last_success_index != len(success) - 1:
                truncated_count += 1
            avg_episode_length += last_success_index + 1

            def recursive_copy_and_slice(
                key, source_group, target_group, add_last_frame=False
            ):
                if isinstance(target_group, h5py.Dataset):
                    if not add_last_frame and ("obs" == key or "env_states" == key):
                        add_last_frame = True
                    source_group.create_dataset(
                        key,
                        data=target_group[: last_success_index + 1 + add_last_frame],
                    )
                elif isinstance(target_group, h5py.Group):
                    if not add_last_frame and ("obs" == key or "env_states" == key):
                        add_last_frame = True
                    source_group.create_group(key, track_order=True)
                    for k in target_group.keys():
                        recursive_copy_and_slice(
                            k,
                            source_group[key],
                            target_group[k],
                            add_last_frame=add_last_frame,
                        )

            recursive_copy_and_slice(traj_id, out_file, traj)
            new_episode = copy.deepcopy(episode)
            new_episode["success"] = True
            new_episode["episode_length"] = last_success_index + 1
            new_metadata["episodes"].append(new_episode)
        final_episode_count = len(new_metadata["episodes"])
        avg_episode_length /= final_episode_count
        avg_steps_to_first_success = np.mean(first_success_indexes)
        print(
            f"{env_id}: Failed: {failed_count}/{original_episode_count}, Truncated: {truncated_count}/{original_episode_count}, Final Episodes: {final_episode_count}, Avg Episode Length: {avg_episode_length}, Avg Steps to First Success: {avg_steps_to_first_success}"
        )

        new_metadata["source_type"] = "rl"
        new_metadata[
            "source_desc"
        ] = "Demonstrations generated by rolling out a PPO dense reward trained policy"
        with open(
            os.path.join(
                args.out_dir,
                f"{env_id}/rl/trajectory.state.pd_joint_delta_pos.cuda.json",
            ),
            "w",
        ) as f:
            json.dump(new_metadata, f, indent=2)
        print(
            f"Saved to {os.path.join(args.out_dir, f'{env_id}/rl/trajectory.state.pd_joint_delta_pos.cuda.json')}"
        )
        out_file.close()


if __name__ == "__main__":
    main()
